#include "OpenVINOEngine.hpp"
#include <inference_engine.hpp>
#include <FAST/Utility.hpp>

namespace fast {

using namespace InferenceEngine;

void OpenVINOEngine::run() {
	try {
		// Copy input data
		reportInfo() << "OpenVINO: Processing input nodes.." << reportEnd();
		int batchSize = -1;
		for(const auto& node : mInputNodes) {
			auto tensor = node.second.data;
			batchSize = tensor->getShape()[0];
			auto access = tensor->getAccess(ACCESS_READ);
			float* tensorData = access->getRawData();
			Blob::Ptr input = m_inferRequest->GetBlob(node.first);

			// Dynamic batch size
			if(m_maxBatchSize > 1)
                m_inferRequest->SetBatch(batchSize);

			auto input_data = input->buffer().as<PrecisionTrait<Precision::FP32>::value_type * >();
			std::memcpy(input_data, tensorData, input->byteSize());
		}
		reportInfo() << "OpenVINO: Finished processing input nodes." << reportEnd();

		// Execute network
        m_inferRequest->Infer();
		reportInfo() << "OpenVINO: Network executed." << reportEnd();

		// Copy output data
		for (auto& node : mOutputNodes) {
			Blob::Ptr output = m_inferRequest->GetBlob(node.first);
			auto outputData = (output->buffer().as<::InferenceEngine::PrecisionTrait<Precision::FP32>::value_type *>());
			auto copied_data = make_uninitialized_unique<float[]>(output->byteSize());
			std::memcpy(copied_data.get(), outputData, output->byteSize());
			auto tensor = Tensor::New();
			tensor->create(std::move(copied_data), node.second.shape);
			node.second.data = tensor;
		}
		reportInfo() << "OpenVINO: Finished processing output nodes." << reportEnd();
	}
	catch (::InferenceEngine::details::InferenceEngineException &e) {
		throw Exception("Inference error occured during OpenVINO::run: " + std::string(e.what()));
	}
}

void OpenVINOEngine::loadPlugin(std::string deviceType) {
    PluginDispatcher dispatcher({""});
    m_inferencePlugin = std::make_shared<InferencePlugin>(dispatcher.getPluginByDevice(deviceType));

    // Load CPU extension if device type is CPU
    if(deviceType == "CPU") {
#ifdef WIN32
        auto extension_ptr = make_so_pointer<::InferenceEngine::IExtension>("cpu_extension.dll");
#else
        auto extension_ptr = make_so_pointer<::InferenceEngine::IExtension>("libcpu_extension.so");
#endif
        m_inferencePlugin->AddExtension(extension_ptr);
    }

    reportInfo() << "OpenVINO: Inference plugin setup complete for device type " << deviceType << reportEnd();

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    auto input_model = getFilename();
    CNNNetReader network_reader;
    if(input_model.empty()) { // If filename is not set, load from memory instead
        // Read from memory
        network_reader.ReadNetwork(m_model.data(), m_model.size());
        network_reader.SetWeights(make_shared_blob<uint8_t>({ Precision::U8, {m_weights.size()}, C }, m_weights.data()));
    } else {
        // Read from file
        if (!fileExists(input_model))
            throw FileNotFoundException(input_model);
        network_reader.ReadNetwork(fileNameToString(input_model));
        network_reader.ReadWeights(fileNameToString(input_model).substr(0, input_model.size() - 4) + ".bin");
    }

    CNNNetwork network = network_reader.getNetwork();
    //network.setBatchSize(1);
    reportInfo() << "OpenVINO: Network loaded." << reportEnd();

    // --------------------------- Prepare input blobs -----------------------------------------------------
    int counter = 0;
    for(auto& input : network.getInputsInfo()) {
        auto input_info = input.second;
        auto input_name = input.first;
        input_info->setPrecision(Precision::FP32);
        // TODO shape is reverse direction here for some reason..
        TensorShape shape;
        auto dims = input_info->getDims();
        for(int i = dims.size() - 1; i >= 0; --i) // TODO why reverse??
            shape.addDimension(dims[i]);

        if(shape.getDimensions() > 3) {
            input_info->setLayout(Layout::NCHW);
            addInputNode(counter, input_name, NodeType::IMAGE, shape);
        } else {
            addInputNode(counter, input_name, NodeType::TENSOR, shape);
        }
        reportInfo() << "Found input node: " << input_name << " with shape " << shape.toString() << reportEnd();
        counter++;
    }

    // --------------------------- Prepare output blobs ----------------------------------------------------
    counter = 0;
    for(auto& output : network.getOutputsInfo()) {
        auto info = output.second;
        auto name = output.first;
        info->setPrecision(Precision::FP32);
        TensorShape shape;
        for(auto dim : info->getDims())
            shape.addDimension(dim);
        addOutputNode(counter, name, NodeType::TENSOR, shape);
        reportInfo() << "Found output node: " << name << " with shape " << shape.toString() << reportEnd();
        counter++;
    }
    reportInfo() << "OpenVINO: Node setup complete." << reportEnd();

    std::map<std::string, std::string> config;
    if(m_maxBatchSize > 1) {
        config[PluginConfigParams::KEY_DYN_BATCH_ENABLED] = PluginConfigParams::YES;
        network.setBatchSize(m_maxBatchSize);
    }

    ExecutableNetwork executable_network = m_inferencePlugin->LoadNetwork(network, config);

    m_inferRequest = executable_network.CreateInferRequestPtr();
    setIsLoaded(true);
    reportInfo() << "OpenVINO: Network fully loaded." << reportEnd();
}

void OpenVINOEngine::load() {
    if(m_deviceType == InferenceDeviceType::ANY) {
        try {
            loadPlugin(getDeviceName(TargetDevice::eGPU));
        } catch(::InferenceEngine::details::InferenceEngineException &e) {
            try {
                reportInfo() << "Failed to get GPU plugin for OpenVINO inference engine: " << e.what() << reportEnd();
                reportInfo() << "Trying VPU/Myriad/Neural compute stick plugin instead.." << reportEnd();
                loadPlugin(getDeviceName(TargetDevice::eMYRIAD));
            } catch(::InferenceEngine::details::InferenceEngineException &e) {
                try {
                    reportInfo() << "Failed to get GPU/VPU plugin for OpenVINO inference engine: " << e.what()
                                 << reportEnd();
                    reportInfo() << "Trying CPU plugin instead.." << reportEnd();
                    loadPlugin(getDeviceName(TargetDevice::eCPU));
                } catch(::InferenceEngine::details::InferenceEngineException &e) {
                    reportError() << e.what() << reportEnd();
                    throw Exception("Failed to load any device in OpenVINO IE");
                }
            }
        }
    } else {
        std::map<InferenceDeviceType, TargetDevice> deviceMapping = {
                {InferenceDeviceType::GPU, TargetDevice::eGPU},
                {InferenceDeviceType::CPU, TargetDevice::eCPU},
                {InferenceDeviceType::VPU, TargetDevice::eMYRIAD},
        };

        try {
            loadPlugin(getDeviceName(deviceMapping[m_deviceType]));
        } catch(::InferenceEngine::details::InferenceEngineException &e) {
            throw Exception(std::string("Failed to load device ") + getDeviceName(deviceMapping[m_deviceType]) + " in OpenVINO inference engine");
        }
    }
}

ImageOrdering OpenVINOEngine::getPreferredImageOrdering() const {
    return ImageOrdering::ChannelFirst;
}

std::string OpenVINOEngine::getName() const {
    return "OpenVINO";
}

std::string OpenVINOEngine::getDefaultFileExtension() const {
    return "xml";
}

OpenVINOEngine::~OpenVINOEngine() {
    //if(m_inferState != nullptr)
    //    delete m_inferState;
}

}
